{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll go through the code for the coding companion for Intuitive Deep Learning Part 1 ([Part 1a](https://medium.com/intuitive-deep-learning/intuitive-deep-learning-part-1a-introduction-to-neural-networks-d7b16ebf6b99), [Part 1b](https://medium.com/intuitive-deep-learning/intuitive-deep-learning-part-1b-introduction-to-neural-networks-8565d97ddd2d)) to create your very first neural network to predict whether the house price is below or above median value. We will go through the following in this notebook:\n",
    "\n",
    "- Exploring and Processing the Data\n",
    "- Building and Training our Neural Network\n",
    "- Visualizing Loss and Accuracy\n",
    "- Adding Regularization to our Neural Network\n",
    "\n",
    "The code is annotated throughout the notebook and you simply need to download the dataset [here](https://drive.google.com/file/d/1GfvKA0qznNVknghV4botnNxyH-KvODOC/view), put the dataset in the same folder as this notebook and run the code cells below. Note that the results you get might differ slightly from the blogpost as there is a degree of randomness in the way we split our dataset as well as the initialization of our neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring and Processing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first have to read in the CSV file that we've been given. We'll use a package called pandas for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('housepricedata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>HalfBath</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>AboveMedianPrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>856</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>548</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1262</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>460</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>920</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>608</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>756</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>642</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1145</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>836</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14115</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>796</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>480</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10084</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1686</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>636</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10382</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1107</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>484</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6120</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>952</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>468</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7420</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>991</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>205</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11200</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1040</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11924</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>1175</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>736</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12968</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>912</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>352</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10652</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1494</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>840</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10920</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1253</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>352</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6120</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>832</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>576</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>11241</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>1004</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>480</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10791</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>516</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>13695</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1114</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>576</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7560</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1029</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>294</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>14215</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1158</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>853</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7449</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>637</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>280</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>9742</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1777</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>534</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4224</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1040</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>572</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>8246</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>1060</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>270</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>14230</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1566</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7200</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>900</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>576</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>11478</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1704</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>772</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>16321</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1484</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>319</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6324</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>520</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>240</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>21930</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>732</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>372</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>4928</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>958</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>10800</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>656</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>216</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>10261</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>936</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>451</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>17400</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1126</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>484</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>8400</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>1319</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>462</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>9000</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>864</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>528</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>12444</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1932</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>774</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>7407</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>912</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>923</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>11584</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>539</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>550</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>11526</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>588</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>672</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>4426</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>848</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>420</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>11003</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>1017</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>812</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>8854</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>952</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>192</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>8500</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1422</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>626</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>8400</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>814</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>240</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>26142</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1188</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>312</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>10000</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1220</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>556</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>11767</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>560</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1449</th>\n",
       "      <td>1533</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>630</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>9000</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>896</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>9262</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1573</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>840</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>3675</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>547</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>525</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>17217</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1140</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>7500</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1221</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>400</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>7917</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>953</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>460</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>13175</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1542</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>9042</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>1152</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>252</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>9717</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1078</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>240</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>9937</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1256</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>276</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      LotArea  OverallQual  OverallCond  TotalBsmtSF  FullBath  HalfBath  \\\n",
       "0        8450            7            5          856         2         1   \n",
       "1        9600            6            8         1262         2         0   \n",
       "2       11250            7            5          920         2         1   \n",
       "3        9550            7            5          756         1         0   \n",
       "4       14260            8            5         1145         2         1   \n",
       "5       14115            5            5          796         1         1   \n",
       "6       10084            8            5         1686         2         0   \n",
       "7       10382            7            6         1107         2         1   \n",
       "8        6120            7            5          952         2         0   \n",
       "9        7420            5            6          991         1         0   \n",
       "10      11200            5            5         1040         1         0   \n",
       "11      11924            9            5         1175         3         0   \n",
       "12      12968            5            6          912         1         0   \n",
       "13      10652            7            5         1494         2         0   \n",
       "14      10920            6            5         1253         1         1   \n",
       "15       6120            7            8          832         1         0   \n",
       "16      11241            6            7         1004         1         0   \n",
       "17      10791            4            5            0         2         0   \n",
       "18      13695            5            5         1114         1         1   \n",
       "19       7560            5            6         1029         1         0   \n",
       "20      14215            8            5         1158         3         1   \n",
       "21       7449            7            7          637         1         0   \n",
       "22       9742            8            5         1777         2         0   \n",
       "23       4224            5            7         1040         1         0   \n",
       "24       8246            5            8         1060         1         0   \n",
       "25      14230            8            5         1566         2         0   \n",
       "26       7200            5            7          900         1         0   \n",
       "27      11478            8            5         1704         2         0   \n",
       "28      16321            5            6         1484         1         0   \n",
       "29       6324            4            6          520         1         0   \n",
       "...       ...          ...          ...          ...       ...       ...   \n",
       "1430    21930            5            5          732         2         1   \n",
       "1431     4928            6            6          958         2         0   \n",
       "1432    10800            4            6          656         2         0   \n",
       "1433    10261            6            5          936         2         1   \n",
       "1434    17400            5            5         1126         2         0   \n",
       "1435     8400            6            9         1319         1         1   \n",
       "1436     9000            4            6          864         1         0   \n",
       "1437    12444            8            5         1932         2         0   \n",
       "1438     7407            6            7          912         1         0   \n",
       "1439    11584            7            6          539         2         1   \n",
       "1440    11526            6            7          588         2         0   \n",
       "1441     4426            6            5          848         1         0   \n",
       "1442    11003           10            5         1017         2         1   \n",
       "1443     8854            6            6          952         1         0   \n",
       "1444     8500            7            5         1422         2         0   \n",
       "1445     8400            6            5          814         1         0   \n",
       "1446    26142            5            7         1188         1         0   \n",
       "1447    10000            8            5         1220         2         1   \n",
       "1448    11767            4            7          560         1         1   \n",
       "1449     1533            5            7          630         1         0   \n",
       "1450     9000            5            5          896         2         2   \n",
       "1451     9262            8            5         1573         2         0   \n",
       "1452     3675            5            5          547         1         0   \n",
       "1453    17217            5            5         1140         1         0   \n",
       "1454     7500            7            5         1221         2         0   \n",
       "1455     7917            6            5          953         2         1   \n",
       "1456    13175            6            6         1542         2         0   \n",
       "1457     9042            7            9         1152         2         0   \n",
       "1458     9717            5            6         1078         1         0   \n",
       "1459     9937            5            6         1256         1         1   \n",
       "\n",
       "      BedroomAbvGr  TotRmsAbvGrd  Fireplaces  GarageArea  AboveMedianPrice  \n",
       "0                3             8           0         548                 1  \n",
       "1                3             6           1         460                 1  \n",
       "2                3             6           1         608                 1  \n",
       "3                3             7           1         642                 0  \n",
       "4                4             9           1         836                 1  \n",
       "5                1             5           0         480                 0  \n",
       "6                3             7           1         636                 1  \n",
       "7                3             7           2         484                 1  \n",
       "8                2             8           2         468                 0  \n",
       "9                2             5           2         205                 0  \n",
       "10               3             5           0         384                 0  \n",
       "11               4            11           2         736                 1  \n",
       "12               2             4           0         352                 0  \n",
       "13               3             7           1         840                 1  \n",
       "14               2             5           1         352                 0  \n",
       "15               2             5           0         576                 0  \n",
       "16               2             5           1         480                 0  \n",
       "17               2             6           0         516                 0  \n",
       "18               3             6           0         576                 0  \n",
       "19               3             6           0         294                 0  \n",
       "20               4             9           1         853                 1  \n",
       "21               3             6           1         280                 0  \n",
       "22               3             7           1         534                 1  \n",
       "23               3             6           1         572                 0  \n",
       "24               3             6           1         270                 0  \n",
       "25               3             7           1         890                 1  \n",
       "26               3             5           0         576                 0  \n",
       "27               3             7           1         772                 1  \n",
       "28               2             6           2         319                 1  \n",
       "29               1             4           0         240                 0  \n",
       "...            ...           ...         ...         ...               ...  \n",
       "1430             4             7           1         372                 1  \n",
       "1431             2             5           0         440                 0  \n",
       "1432             4             5           0         216                 0  \n",
       "1433             3             8           1         451                 1  \n",
       "1434             3             5           1         484                 0  \n",
       "1435             3             7           1         462                 1  \n",
       "1436             3             5           0         528                 0  \n",
       "1437             2             7           1         774                 1  \n",
       "1438             2             6           0         923                 0  \n",
       "1439             3             6           1         550                 1  \n",
       "1440             3            11           1         672                 1  \n",
       "1441             1             3           1         420                 0  \n",
       "1442             3            10           1         812                 1  \n",
       "1443             2             4           1         192                 0  \n",
       "1444             3             7           0         626                 1  \n",
       "1445             3             6           0         240                 0  \n",
       "1446             3             6           0         312                 0  \n",
       "1447             3             8           1         556                 1  \n",
       "1448             2             6           0         384                 0  \n",
       "1449             1             3           0           0                 0  \n",
       "1450             4             8           0           0                 0  \n",
       "1451             3             7           1         840                 1  \n",
       "1452             2             5           0         525                 0  \n",
       "1453             3             6           0           0                 0  \n",
       "1454             2             6           0         400                 1  \n",
       "1455             3             7           1         460                 1  \n",
       "1456             3             7           2         500                 1  \n",
       "1457             4             9           2         252                 1  \n",
       "1458             2             5           0         240                 0  \n",
       "1459             3             6           0         276                 0  \n",
       "\n",
       "[1460 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that we have now is in what we call a pandas dataframe. To convert it to an array, simply access its values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8450,     7,     5, ...,     0,   548,     1],\n",
       "       [ 9600,     6,     8, ...,     1,   460,     1],\n",
       "       [11250,     7,     5, ...,     1,   608,     1],\n",
       "       ...,\n",
       "       [ 9042,     7,     9, ...,     2,   252,     1],\n",
       "       [ 9717,     5,     6, ...,     0,   240,     0],\n",
       "       [ 9937,     5,     6, ...,     0,   276,     0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we split the dataset into our input features and the label we wish to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[:,0:10]\n",
    "Y = dataset[:,10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing our data is very important, as we want the input features to be on the same order of magnitude to make our training easier. We'll use a min-max scaler from scikit-learn which scales our data to be between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dikshya/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_scale = min_max_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0334198 , 0.66666667, 0.5       , ..., 0.5       , 0.        ,\n",
       "        0.3864598 ],\n",
       "       [0.03879502, 0.55555556, 0.875     , ..., 0.33333333, 0.33333333,\n",
       "        0.32440056],\n",
       "       [0.04650728, 0.66666667, 0.5       , ..., 0.33333333, 0.33333333,\n",
       "        0.42877292],\n",
       "       ...,\n",
       "       [0.03618687, 0.66666667, 1.        , ..., 0.58333333, 0.66666667,\n",
       "        0.17771509],\n",
       "       [0.03934189, 0.44444444, 0.625     , ..., 0.25      , 0.        ,\n",
       "        0.16925247],\n",
       "       [0.04037019, 0.44444444, 0.625     , ..., 0.33333333, 0.        ,\n",
       "        0.19464034]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we wish to set aside some parts of our dataset for a validation set and a test set. We use the function train_test_split from scikit-learn to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_scale, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1022, 10) (219, 10) (219, 10) (1022,) (219,) (219,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Training Our First Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using Keras to build our architecture. Let's import the code from Keras that we will need to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /Users/dikshya/anaconda3/lib/python3.6/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /Users/dikshya/anaconda3/lib/python3.6/site-packages (from keras) (1.17.4)\n",
      "Requirement already satisfied: scipy>=0.14 in /Users/dikshya/anaconda3/lib/python3.6/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/dikshya/anaconda3/lib/python3.6/site-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: pyyaml in /Users/dikshya/anaconda3/lib/python3.6/site-packages (from keras) (3.12)\n",
      "Requirement already satisfied: h5py in /Users/dikshya/anaconda3/lib/python3.6/site-packages (from keras) (2.7.1)\n",
      "Collecting keras_applications==1.0.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/90/8f327deaa37a71caddb59b7b4aaa9d4b3e90c0e76f8c2d1572005278ddc5/Keras_Applications-1.0.4-py2.py3-none-any.whl (43kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 1.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras_preprocessing==1.0.2\n",
      "  Downloading https://files.pythonhosted.org/packages/71/26/1e778ebd737032749824d5cba7dbd3b0cf9234b87ab5ec79f5f0403ca7e9/Keras_Preprocessing-1.0.2-py2.py3-none-any.whl\n",
      "Installing collected packages: keras-applications, keras-preprocessing\n",
      "  Found existing installation: Keras-Applications 1.0.8\n",
      "    Uninstalling Keras-Applications-1.0.8:\n",
      "      Successfully uninstalled Keras-Applications-1.0.8\n",
      "  Found existing installation: Keras-Preprocessing 1.1.0\n",
      "    Uninstalling Keras-Preprocessing-1.1.0:\n",
      "      Successfully uninstalled Keras-Preprocessing-1.1.0\n",
      "Successfully installed keras-applications-1.0.4 keras-preprocessing-1.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the Sequential model, which means that we merely need to describe the layers above in sequence. Our neural network has three layers:\n",
    "\n",
    "- Hidden layer 1: 30 neurons, ReLU activation\n",
    "- Hidden layer 2: 30 neurons, ReLU activation\n",
    "- Output Layer: 1 neuron, Sigmoid activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=(10,)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got our architecture specified, we need to find the best numbers for it. Before we start our training, we have to configure the model by\n",
    "- Telling it what algorithm you want to use to do the optimization (we'll use stochastic gradient descent)\n",
    "- Telling it what loss function to use (for binary classification, we will use binary cross entropy)\n",
    "- Telling it what other metrics you want to track apart from the loss function (we want to track accuracy as well)\n",
    "\n",
    "We do so below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on the data is pretty straightforward and requires us to write one line of code. The function is called 'fit' as we are fitting the parameters to the data. We specify:\n",
    "- what data we are training on, which is X_train and Y_train\n",
    "- the size of our mini-batch \n",
    "- how long we want to train it for (epochs)\n",
    "- what our validation data is so that the model will tell us how we are doing on the validation data at each point.\n",
    "\n",
    "This function will output a history, which we save under the variable hist. We'll use this variable a little later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1022 samples, validate on 219 samples\n",
      "Epoch 1/100\n",
      "1022/1022 [==============================] - 0s 352us/step - loss: 0.6934 - acc: 0.5098 - val_loss: 0.7009 - val_acc: 0.3653\n",
      "Epoch 2/100\n",
      "1022/1022 [==============================] - 0s 62us/step - loss: 0.6811 - acc: 0.5157 - val_loss: 0.6979 - val_acc: 0.3653\n",
      "Epoch 3/100\n",
      "1022/1022 [==============================] - 0s 53us/step - loss: 0.6709 - acc: 0.5157 - val_loss: 0.6940 - val_acc: 0.3653\n",
      "Epoch 4/100\n",
      "1022/1022 [==============================] - 0s 53us/step - loss: 0.6623 - acc: 0.5157 - val_loss: 0.6893 - val_acc: 0.3653\n",
      "Epoch 5/100\n",
      "1022/1022 [==============================] - 0s 50us/step - loss: 0.6551 - acc: 0.5176 - val_loss: 0.6840 - val_acc: 0.3744\n",
      "Epoch 6/100\n",
      "1022/1022 [==============================] - 0s 49us/step - loss: 0.6481 - acc: 0.5391 - val_loss: 0.6778 - val_acc: 0.4155\n",
      "Epoch 7/100\n",
      "1022/1022 [==============================] - 0s 50us/step - loss: 0.6414 - acc: 0.5949 - val_loss: 0.6714 - val_acc: 0.4932\n",
      "Epoch 8/100\n",
      "1022/1022 [==============================] - 0s 49us/step - loss: 0.6347 - acc: 0.6703 - val_loss: 0.6648 - val_acc: 0.5434\n",
      "Epoch 9/100\n",
      "1022/1022 [==============================] - 0s 49us/step - loss: 0.6277 - acc: 0.6967 - val_loss: 0.6575 - val_acc: 0.5936\n",
      "Epoch 10/100\n",
      "1022/1022 [==============================] - 0s 50us/step - loss: 0.6205 - acc: 0.7231 - val_loss: 0.6500 - val_acc: 0.6164\n",
      "Epoch 11/100\n",
      "1022/1022 [==============================] - 0s 50us/step - loss: 0.6132 - acc: 0.7524 - val_loss: 0.6437 - val_acc: 0.6347\n",
      "Epoch 12/100\n",
      "1022/1022 [==============================] - 0s 49us/step - loss: 0.6054 - acc: 0.7515 - val_loss: 0.6350 - val_acc: 0.6575\n",
      "Epoch 13/100\n",
      "1022/1022 [==============================] - 0s 53us/step - loss: 0.5978 - acc: 0.7730 - val_loss: 0.6286 - val_acc: 0.6667\n",
      "Epoch 14/100\n",
      "1022/1022 [==============================] - 0s 52us/step - loss: 0.5898 - acc: 0.7769 - val_loss: 0.6211 - val_acc: 0.6941\n",
      "Epoch 15/100\n",
      "1022/1022 [==============================] - 0s 53us/step - loss: 0.5819 - acc: 0.8004 - val_loss: 0.6147 - val_acc: 0.7078\n",
      "Epoch 16/100\n",
      "1022/1022 [==============================] - 0s 50us/step - loss: 0.5737 - acc: 0.8072 - val_loss: 0.6076 - val_acc: 0.7123\n",
      "Epoch 17/100\n",
      "1022/1022 [==============================] - 0s 53us/step - loss: 0.5654 - acc: 0.8102 - val_loss: 0.5993 - val_acc: 0.7306\n",
      "Epoch 18/100\n",
      "1022/1022 [==============================] - 0s 49us/step - loss: 0.5570 - acc: 0.8200 - val_loss: 0.5916 - val_acc: 0.7443\n",
      "Epoch 19/100\n",
      "1022/1022 [==============================] - 0s 50us/step - loss: 0.5484 - acc: 0.8219 - val_loss: 0.5813 - val_acc: 0.7580\n",
      "Epoch 20/100\n",
      "1022/1022 [==============================] - 0s 50us/step - loss: 0.5396 - acc: 0.8327 - val_loss: 0.5760 - val_acc: 0.7580\n",
      "Epoch 21/100\n",
      "1022/1022 [==============================] - 0s 51us/step - loss: 0.5309 - acc: 0.8317 - val_loss: 0.5661 - val_acc: 0.7626\n",
      "Epoch 22/100\n",
      "1022/1022 [==============================] - 0s 52us/step - loss: 0.5219 - acc: 0.8317 - val_loss: 0.5570 - val_acc: 0.7671\n",
      "Epoch 23/100\n",
      "1022/1022 [==============================] - 0s 70us/step - loss: 0.5130 - acc: 0.8376 - val_loss: 0.5499 - val_acc: 0.7671\n",
      "Epoch 24/100\n",
      "1022/1022 [==============================] - 0s 70us/step - loss: 0.5042 - acc: 0.8415 - val_loss: 0.5402 - val_acc: 0.7717\n",
      "Epoch 25/100\n",
      "1022/1022 [==============================] - 0s 75us/step - loss: 0.4953 - acc: 0.8395 - val_loss: 0.5297 - val_acc: 0.7808\n",
      "Epoch 26/100\n",
      "1022/1022 [==============================] - 0s 68us/step - loss: 0.4865 - acc: 0.8474 - val_loss: 0.5250 - val_acc: 0.7763\n",
      "Epoch 27/100\n",
      "1022/1022 [==============================] - 0s 65us/step - loss: 0.4779 - acc: 0.8483 - val_loss: 0.5169 - val_acc: 0.7763\n",
      "Epoch 28/100\n",
      "1022/1022 [==============================] - 0s 60us/step - loss: 0.4692 - acc: 0.8493 - val_loss: 0.5102 - val_acc: 0.7763\n",
      "Epoch 29/100\n",
      "1022/1022 [==============================] - 0s 65us/step - loss: 0.4609 - acc: 0.8474 - val_loss: 0.4971 - val_acc: 0.8128\n",
      "Epoch 30/100\n",
      "1022/1022 [==============================] - 0s 65us/step - loss: 0.4529 - acc: 0.8523 - val_loss: 0.4901 - val_acc: 0.8128\n",
      "Epoch 31/100\n",
      "1022/1022 [==============================] - 0s 66us/step - loss: 0.4448 - acc: 0.8493 - val_loss: 0.4829 - val_acc: 0.8128\n",
      "Epoch 32/100\n",
      "1022/1022 [==============================] - 0s 70us/step - loss: 0.4369 - acc: 0.8542 - val_loss: 0.4747 - val_acc: 0.8128\n",
      "Epoch 33/100\n",
      "1022/1022 [==============================] - 0s 67us/step - loss: 0.4295 - acc: 0.8601 - val_loss: 0.4707 - val_acc: 0.8128\n",
      "Epoch 34/100\n",
      "1022/1022 [==============================] - 0s 65us/step - loss: 0.4221 - acc: 0.8601 - val_loss: 0.4602 - val_acc: 0.8174\n",
      "Epoch 35/100\n",
      "1022/1022 [==============================] - 0s 63us/step - loss: 0.4152 - acc: 0.8650 - val_loss: 0.4554 - val_acc: 0.8174\n",
      "Epoch 36/100\n",
      "1022/1022 [==============================] - 0s 59us/step - loss: 0.4085 - acc: 0.8630 - val_loss: 0.4489 - val_acc: 0.8356\n",
      "Epoch 37/100\n",
      "1022/1022 [==============================] - 0s 49us/step - loss: 0.4021 - acc: 0.8659 - val_loss: 0.4393 - val_acc: 0.8493\n",
      "Epoch 38/100\n",
      "1022/1022 [==============================] - 0s 50us/step - loss: 0.3958 - acc: 0.8679 - val_loss: 0.4395 - val_acc: 0.8402\n",
      "Epoch 39/100\n",
      "1022/1022 [==============================] - 0s 48us/step - loss: 0.3898 - acc: 0.8659 - val_loss: 0.4300 - val_acc: 0.8493\n",
      "Epoch 40/100\n",
      "1022/1022 [==============================] - 0s 49us/step - loss: 0.3846 - acc: 0.8679 - val_loss: 0.4211 - val_acc: 0.8630\n",
      "Epoch 41/100\n",
      "1022/1022 [==============================] - 0s 51us/step - loss: 0.3792 - acc: 0.8650 - val_loss: 0.4190 - val_acc: 0.8584\n",
      "Epoch 42/100\n",
      "1022/1022 [==============================] - 0s 47us/step - loss: 0.3739 - acc: 0.8650 - val_loss: 0.4175 - val_acc: 0.8493\n",
      "Epoch 43/100\n",
      "1022/1022 [==============================] - 0s 48us/step - loss: 0.3691 - acc: 0.8718 - val_loss: 0.4062 - val_acc: 0.8721\n",
      "Epoch 44/100\n",
      "1022/1022 [==============================] - 0s 47us/step - loss: 0.3647 - acc: 0.8699 - val_loss: 0.4025 - val_acc: 0.8721\n",
      "Epoch 45/100\n",
      "1022/1022 [==============================] - 0s 48us/step - loss: 0.3602 - acc: 0.8689 - val_loss: 0.3991 - val_acc: 0.8721\n",
      "Epoch 46/100\n",
      "1022/1022 [==============================] - 0s 53us/step - loss: 0.3566 - acc: 0.8689 - val_loss: 0.3942 - val_acc: 0.8721\n",
      "Epoch 47/100\n",
      "1022/1022 [==============================] - 0s 50us/step - loss: 0.3525 - acc: 0.8689 - val_loss: 0.3998 - val_acc: 0.8630\n",
      "Epoch 48/100\n",
      "1022/1022 [==============================] - 0s 49us/step - loss: 0.3494 - acc: 0.8708 - val_loss: 0.3916 - val_acc: 0.8676\n",
      "Epoch 49/100\n",
      "1022/1022 [==============================] - 0s 50us/step - loss: 0.3453 - acc: 0.8679 - val_loss: 0.3889 - val_acc: 0.8676\n",
      "Epoch 50/100\n",
      "1022/1022 [==============================] - 0s 49us/step - loss: 0.3417 - acc: 0.8738 - val_loss: 0.3786 - val_acc: 0.8721\n",
      "Epoch 51/100\n",
      "1022/1022 [==============================] - 0s 47us/step - loss: 0.3388 - acc: 0.8708 - val_loss: 0.3844 - val_acc: 0.8676\n",
      "Epoch 52/100\n",
      "1022/1022 [==============================] - 0s 48us/step - loss: 0.3358 - acc: 0.8699 - val_loss: 0.3835 - val_acc: 0.8676\n",
      "Epoch 53/100\n",
      "1022/1022 [==============================] - 0s 48us/step - loss: 0.3328 - acc: 0.8689 - val_loss: 0.3795 - val_acc: 0.8676\n",
      "Epoch 54/100\n",
      "1022/1022 [==============================] - 0s 49us/step - loss: 0.3303 - acc: 0.8689 - val_loss: 0.3801 - val_acc: 0.8676\n",
      "Epoch 55/100\n",
      "1022/1022 [==============================] - 0s 49us/step - loss: 0.3281 - acc: 0.8689 - val_loss: 0.3740 - val_acc: 0.8721\n",
      "Epoch 56/100\n",
      "1022/1022 [==============================] - 0s 49us/step - loss: 0.3256 - acc: 0.8689 - val_loss: 0.3684 - val_acc: 0.8767\n",
      "Epoch 57/100\n",
      "1022/1022 [==============================] - 0s 49us/step - loss: 0.3226 - acc: 0.8699 - val_loss: 0.3713 - val_acc: 0.8721\n",
      "Epoch 58/100\n",
      "1022/1022 [==============================] - 0s 47us/step - loss: 0.3206 - acc: 0.8718 - val_loss: 0.3640 - val_acc: 0.8767\n",
      "Epoch 59/100\n",
      "1022/1022 [==============================] - 0s 47us/step - loss: 0.3187 - acc: 0.8718 - val_loss: 0.3592 - val_acc: 0.8767\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1022/1022 [==============================] - 0s 48us/step - loss: 0.3172 - acc: 0.8708 - val_loss: 0.3567 - val_acc: 0.8767\n",
      "Epoch 61/100\n",
      "1022/1022 [==============================] - 0s 48us/step - loss: 0.3153 - acc: 0.8728 - val_loss: 0.3560 - val_acc: 0.8767\n",
      "Epoch 62/100\n",
      "1022/1022 [==============================] - 0s 59us/step - loss: 0.3127 - acc: 0.8757 - val_loss: 0.3465 - val_acc: 0.8767\n",
      "Epoch 63/100\n",
      "1022/1022 [==============================] - 0s 65us/step - loss: 0.3117 - acc: 0.8738 - val_loss: 0.3500 - val_acc: 0.8767\n",
      "Epoch 64/100\n",
      "1022/1022 [==============================] - 0s 68us/step - loss: 0.3097 - acc: 0.8738 - val_loss: 0.3485 - val_acc: 0.8767\n",
      "Epoch 65/100\n",
      "1022/1022 [==============================] - 0s 76us/step - loss: 0.3073 - acc: 0.8748 - val_loss: 0.3567 - val_acc: 0.8721\n",
      "Epoch 66/100\n",
      "1022/1022 [==============================] - 0s 54us/step - loss: 0.3064 - acc: 0.8689 - val_loss: 0.3419 - val_acc: 0.8858\n",
      "Epoch 67/100\n",
      "1022/1022 [==============================] - 0s 51us/step - loss: 0.3051 - acc: 0.8826 - val_loss: 0.3421 - val_acc: 0.8858\n",
      "Epoch 68/100\n",
      "1022/1022 [==============================] - 0s 52us/step - loss: 0.3030 - acc: 0.8787 - val_loss: 0.3536 - val_acc: 0.8676\n",
      "Epoch 69/100\n",
      "1022/1022 [==============================] - 0s 51us/step - loss: 0.3019 - acc: 0.8777 - val_loss: 0.3513 - val_acc: 0.8676\n",
      "Epoch 70/100\n",
      "1022/1022 [==============================] - 0s 47us/step - loss: 0.3012 - acc: 0.8806 - val_loss: 0.3450 - val_acc: 0.8767\n",
      "Epoch 71/100\n",
      "1022/1022 [==============================] - 0s 47us/step - loss: 0.2992 - acc: 0.8777 - val_loss: 0.3384 - val_acc: 0.8858\n",
      "Epoch 72/100\n",
      "1022/1022 [==============================] - 0s 47us/step - loss: 0.2979 - acc: 0.8826 - val_loss: 0.3534 - val_acc: 0.8630\n",
      "Epoch 73/100\n",
      "1022/1022 [==============================] - 0s 49us/step - loss: 0.2968 - acc: 0.8767 - val_loss: 0.3313 - val_acc: 0.8858\n",
      "Epoch 74/100\n",
      "1022/1022 [==============================] - 0s 55us/step - loss: 0.2953 - acc: 0.8767 - val_loss: 0.3449 - val_acc: 0.8721\n",
      "Epoch 75/100\n",
      "1022/1022 [==============================] - 0s 67us/step - loss: 0.2948 - acc: 0.8836 - val_loss: 0.3364 - val_acc: 0.8858\n",
      "Epoch 76/100\n",
      "1022/1022 [==============================] - 0s 68us/step - loss: 0.2931 - acc: 0.8806 - val_loss: 0.3286 - val_acc: 0.8858\n",
      "Epoch 77/100\n",
      "1022/1022 [==============================] - 0s 76us/step - loss: 0.2925 - acc: 0.8816 - val_loss: 0.3301 - val_acc: 0.8858\n",
      "Epoch 78/100\n",
      "1022/1022 [==============================] - 0s 64us/step - loss: 0.2914 - acc: 0.8816 - val_loss: 0.3319 - val_acc: 0.8858\n",
      "Epoch 79/100\n",
      "1022/1022 [==============================] - 0s 53us/step - loss: 0.2902 - acc: 0.8836 - val_loss: 0.3299 - val_acc: 0.8858\n",
      "Epoch 80/100\n",
      "1022/1022 [==============================] - 0s 49us/step - loss: 0.2894 - acc: 0.8836 - val_loss: 0.3357 - val_acc: 0.8813\n",
      "Epoch 81/100\n",
      "1022/1022 [==============================] - 0s 49us/step - loss: 0.2883 - acc: 0.8806 - val_loss: 0.3357 - val_acc: 0.8813\n",
      "Epoch 82/100\n",
      "1022/1022 [==============================] - 0s 53us/step - loss: 0.2878 - acc: 0.8836 - val_loss: 0.3349 - val_acc: 0.8813\n",
      "Epoch 83/100\n",
      "1022/1022 [==============================] - 0s 48us/step - loss: 0.2864 - acc: 0.8806 - val_loss: 0.3366 - val_acc: 0.8767\n",
      "Epoch 84/100\n",
      "1022/1022 [==============================] - 0s 47us/step - loss: 0.2850 - acc: 0.8836 - val_loss: 0.3239 - val_acc: 0.8858\n",
      "Epoch 85/100\n",
      "1022/1022 [==============================] - 0s 48us/step - loss: 0.2838 - acc: 0.8826 - val_loss: 0.3300 - val_acc: 0.8813\n",
      "Epoch 86/100\n",
      "1022/1022 [==============================] - 0s 47us/step - loss: 0.2836 - acc: 0.8806 - val_loss: 0.3273 - val_acc: 0.8858\n",
      "Epoch 87/100\n",
      "1022/1022 [==============================] - 0s 47us/step - loss: 0.2829 - acc: 0.8806 - val_loss: 0.3175 - val_acc: 0.8904\n",
      "Epoch 88/100\n",
      "1022/1022 [==============================] - 0s 48us/step - loss: 0.2816 - acc: 0.8855 - val_loss: 0.3099 - val_acc: 0.9041\n",
      "Epoch 89/100\n",
      "1022/1022 [==============================] - 0s 49us/step - loss: 0.2812 - acc: 0.8836 - val_loss: 0.3326 - val_acc: 0.8767\n",
      "Epoch 90/100\n",
      "1022/1022 [==============================] - 0s 53us/step - loss: 0.2798 - acc: 0.8836 - val_loss: 0.3127 - val_acc: 0.8995\n",
      "Epoch 91/100\n",
      "1022/1022 [==============================] - 0s 50us/step - loss: 0.2793 - acc: 0.8845 - val_loss: 0.3260 - val_acc: 0.8813\n",
      "Epoch 92/100\n",
      "1022/1022 [==============================] - 0s 50us/step - loss: 0.2782 - acc: 0.8796 - val_loss: 0.3138 - val_acc: 0.8950\n",
      "Epoch 93/100\n",
      "1022/1022 [==============================] - 0s 50us/step - loss: 0.2782 - acc: 0.8845 - val_loss: 0.3228 - val_acc: 0.8813\n",
      "Epoch 94/100\n",
      "1022/1022 [==============================] - 0s 47us/step - loss: 0.2771 - acc: 0.8855 - val_loss: 0.3191 - val_acc: 0.8904\n",
      "Epoch 95/100\n",
      "1022/1022 [==============================] - 0s 47us/step - loss: 0.2774 - acc: 0.8796 - val_loss: 0.3196 - val_acc: 0.8904\n",
      "Epoch 96/100\n",
      "1022/1022 [==============================] - 0s 46us/step - loss: 0.2763 - acc: 0.8855 - val_loss: 0.3243 - val_acc: 0.8767\n",
      "Epoch 97/100\n",
      "1022/1022 [==============================] - 0s 55us/step - loss: 0.2758 - acc: 0.8826 - val_loss: 0.3161 - val_acc: 0.8904\n",
      "Epoch 98/100\n",
      "1022/1022 [==============================] - 0s 53us/step - loss: 0.2745 - acc: 0.8865 - val_loss: 0.3185 - val_acc: 0.8858\n",
      "Epoch 99/100\n",
      "1022/1022 [==============================] - 0s 47us/step - loss: 0.2743 - acc: 0.8845 - val_loss: 0.3106 - val_acc: 0.8995\n",
      "Epoch 100/100\n",
      "1022/1022 [==============================] - 0s 47us/step - loss: 0.2732 - acc: 0.8826 - val_loss: 0.3237 - val_acc: 0.8767\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, Y_train,\n",
    "          batch_size=32, epochs=100,\n",
    "          validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating our data on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 0s 81us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8630136975414677"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, Y_test)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Loss and Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the relevant package we need to do the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to visualize the training loss and the validation loss like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the training accuracy and the validation accuracy like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xd81PX9wPHXOztABpCwEiBMIWyJA0UBNy7cgFqr1Vr7c9TVVq21arW1rbt11IFbcVtUEAVRHKBsZMqGTMLI3rn374/vEUJIyCXc5ZK79/PxyCP3nff+cnrvfLaoKsYYYwxAiL8DMMYY03pYUjDGGFPDkoIxxpgalhSMMcbUsKRgjDGmhiUFY4wxNSwpmKAgIikioiIS5sG5V4rIty0RlzGtjSUF0+qIyFYRqRCRhDr7l7m/2FP8E5kxgc+SgmmttgBT922IyDCgnf/CaR08KekYczgsKZjW6jXgilrbvwRerX2CiMSJyKsikisi20TkbhEJcR8LFZGHRWSXiGwGzqrn2hdFJEtEMkTkAREJ9SQwEXlXRLJFJF9E5ovIkFrHokXkEXc8+SLyrYhEu4+NFZHvRSRPRHaIyJXu/V+JyDW17nFA9ZW7dHS9iGwANrj3PeG+R4GILBGRE2qdHyoid4nIJhEpdB/vKSJPicgjdZ5lhojc4slzm+BgScG0VguBWBEZ7P6yngK8XuecfwNxQF9gHE4Sucp97NfA2cAoIA24qM61LwNVQH/3OacB1+CZWcAAoAuwFHij1rGHgdHAcUAn4A+AS0R6u6/7N5AIjASWe/h+AOcBxwCp7u1F7nt0At4E3hWRKPexW3FKWWcCscCvgBLgFWBqrcSZAJzivt4Yh6raj/20qh9gK86X1d3A34EzgC+AMECBFCAUqABSa133G+Ar9+svgetqHTvNfW0Y0BUoB6JrHZ8KzHO/vhL41sNY4933jcP5I6sUGFHPeXcCHzZwj6+Aa2ptH/D+7vuf1Egce/e9L7AemNTAeWuBU92vbwBm+vvztp/W9WP1k6Y1ew2YD/ShTtURkACEA9tq7dsGJLlf9wB21Dm2T2/3tVkism9fSJ3z6+UutTwIXIzzF7+rVjyRQBSwqZ5Lezaw31MHxCYitwNX4zyn4pQI9jXMH+q9XgEux0mylwNPHEZMJgBZ9ZFptVR1G06D85nAB3UO7wIqcb7g9+kFZLhfZ+F8OdY+ts8OnJJCgqrGu39iVXUIjbsUmIRTkonDKbUAiDumMqBfPdftaGA/QDEHNqJ3q+ecmumM3e0HfwAuATqqajyQ746hsfd6HZgkIiOAwcBHDZxngpQlBdPaXY1TdVJce6eqVgPvAA+KSIy7zv5W9rc7vAPcJCLJItIRuKPWtVnA58AjIhIrIiEi0k9ExnkQTwxOQtmN80X+t1r3dQHTgEdFpIe7wXeMiETitDucIiKXiEiYiHQWkZHuS5cDF4hIOxHp737mxmKoAnKBMBG5B6eksM8LwF9FZIA4hotIZ3eM6TjtEa8B76tqqQfPbIKIJQXTqqnqJlVd3MDhG3H+yt4MfIvTYDrNfex5YDawAqcxuG5J4wogAliDUx//HtDdg5BexamKynBfu7DO8duBn3C+ePcA/wBCVHU7TonnNvf+5cAI9zWP4bSP5OBU77zBoc0GPgN+dsdSxoHVS4/iJMXPgQLgRSC61vFXgGE4icGYA4iqLbJjTDARkRNxSlS91b4ATB1WUjAmiIhIOPA74AVLCKY+lhSMCRIiMhjIw6kme9zP4ZhWyqqPjDHG1LCSgjHGmBptbvBaQkKCpqSk+DsMY4xpU5YsWbJLVRMbO6/NJYWUlBQWL26oh6Ixxpj6iMi2xs+y6iNjjDG1WFIwxhhTw5KCMcaYGpYUjDHG1LCkYIwxpoYlBWOMMTUsKRhjjKnh06QgImeIyHoR2Sgid9RzvLeIzBWRle7Fy5N9GY8xxnjkp/dgr0fd+gOOz5KCe9nCp4CJOIuNTxWR1DqnPQy8qqrDgftx1uM1xhj/2TgH3r8avgrOryNflhSOBjaq6mZVrQCm4yxjWFsqzgLrAPPqOW6MMS2nsgxm/t55/fNscFUfeHz2n+CH51omlsJseHUS7FzXMu/n5sukkMSBq0Gls39R9X1WABe4X58PxOxbNrA2EblWRBaLyOLc3FyfBGuMMXz3OOzZDKOvgtI9sOOH/cfy02HBUzD3fijLP/C66ionoXjTZ3fC5q/gx/96976N8HdD8+3AOBFZBozDWeKwuu5JqvqcqqapalpiYqPzORljTNPt3gTfPApDL4JT74eQcFg/c//xle8AChWFsPil/ftV4fUL4Llx3ksMm76E1R9AZByset/7CecQfJkUMoCetbaT3ftqqGqmql6gqqOAP7n35fkwJmOMOVhFMXxyC4RFwukPQlQs9DkR1s9yjqvCiunQawz0GQcLn4GqcufY8jdhy9eQu84padS2ZzN88wjM/5fzs6ruUuFAeREseRlK9jjblWXw6e3QqS9c8F+nVPLzZz579Lp8OUvqImCAiPTBSQZTgEtrnyAiCcAeVXUBd7J/0XVjjPG96ipY/jrM+zsUZcPZj0FMN+fYERNh5u2wawOUF8Ku9XDOExDX0ykZ/PQuHHEmfPFnSD4a4pKcksawi6FzP9i7FaZNdO5bW1gUDDpz//YXf4bF0+Dze+CEW9GyAmTPJrj8A+g7HmK6OwlpyHkt8k/is6SgqlUicgMwGwgFpqnqahG5H1isqjOA8cDfRUSB+cD1vorHGHMIK6bDwqedv4gBEgfBBc+BSOPXZq+Cr/8BE/8Jsd33768ogfevgbE3Q8+j9+93VcOMmyB7ZdNi7NwPxt8JiUc07brMZU47wIhLYeiFEBLiPOf6WTDnXufLvucxcMkr0OtYMvNKeW7+ZhKqB3ADULJyBu3KciA0ElLPo1jaE9Y5lap5j+La8B0xpXlOMmnXGTbMoWzGbXw+8C+c+M0viKgs5rXU15gy8RTiogSemwCz/shChjJjbT5ndszk+MUvIcMupjB/NzFz/oIAX4WNZfbKREbnZ3HqgPOJXf4cUpQLHXxffd7mluNMS0tTW0/BGC/aswWePhbie0OnvhTl76JDziL2nP8mnUac1fi1006HohwYdwfbht/Emz9u56IjkxmQMws++DV0HgC//c6pmgFyvniCrt/dw8qIkXSMi6NLbBQRoUJFlVJWVU21a/93UmlFNXmlFeSXVDK8ahXRlPGeTmBG5LmMGdSTU1K70iM+mm9+zmXO2hxW7VbSBvfj3BFJjOgZx6LFP3DknKlEuwoJw8WW8P78mHgx40tn03XvUrTzAOSUe2HQWZRVuXh+/mae/moT1S5FUT4MvZMqQugluSzUIdxcfTMV1S4mhXzLExFPA/B9lykMufI/RIaHsPCtBxm/+RGytBNxFHND2F+YX9qHxJhIHrl4BKNlHZGvncXTVefyuGsy74f9iS6Sz+8SnmdhRgUTItfz285LeaPd5XyZLhSWVTFQdvB55B95J/EGep5xK2P6HdQXxyMiskRV0xo9z5KCMUFMFd68BLZ9Dzcsoiy6KxMfncObJdexRbvz0fBnuGJMCv0SOxAdEUq1S1mwaTczVmSwfdtWnq64k1iKkbhkCvP3cHTRI1RUK5FhIXzZ9Ql6FK5CKgrhpLvhxN8zb9FK0j45jdUhA7k39q+syykiNEQIDREqqlz1htixXThpKZ0YEl/B2IyXGLXzA0K1qt5zqwnhPT2Jh8svIEyqeTfiPqKlkmkDnqJL4RrO3Pk8ia5ccjWOx6ou4tOwU4iOdJJVcUUVhWVVnDmsG3dOHExiTCS7Prmf5BVOO8G7Ax9hU8exxEaHkRwbxulzJ1JWUcnxRQ8R0S6O6PBQsvOK+CruPpIrt1I5+U0ijjidlel53Dx9OZt3FdMjLopbih/ngrDvcI2+ivAlL/BGz/t4vWg054zozmXH9CYuOhwAl0v5eWchi7fuZdy8CymucLHhvE84Z0SPZn3UlhSMaWvydjhdIAeeAZEd9u8v3euMsK0s8fhWldUuQsT5sj1I9xFOY6kIrJkB7/wCTv8bjLmeRz5fz7+/3MjHRy5l2JqHuaDqbyytSgGgU/sIAPYUV9AtsoLp4ffTpTKDyyruon9oDv8Ke5one/+bk06bxLRZ3/OvHVOZEXspI6Ky6bX7W54c9Dr9fnqMM0N/pOCqb+jcO5V12QXMXJlFeZWLHvHRdI+LIiYqvCbUxJhI+iW2R2pXY+3dCtsWUFxRyU/p+ewurmBoUhy9OkYjmcvQJS9RLREUhcbSQYuQKz8lNGmk+x+mDN2+gB3thrAos4IV6XmUVzrJKCREOGd4d47rn7D/vbJWwH9PhHYJcNs6CN0fG3nbQUJZXdyBv89cR3FFFX84fRBjurqcdoRuw2pOLamo4u8z1zF/Qy4PntaNsbPOgLI86DsBfvFh49V0C56G2Xfium4BId3qjgH2jCUFY1rI2qwCSiurGdUz/sAvL0+V7oVvH4OFz0J1ObTvAuPvgOGTnQbIbx4+uF/8Ydock8b7MZfxm11/p0N8IiG/mc+mPWVMfPwbzhrenccm9YXHhlCechKzBv2NjLxSMvNKKa2s5vSBcZy69P8ISf+R4gvf5NPSVFZtzuAvP59P6PCL4dwn0W+fQObcw0TX4+RVhDIn8nYyNIGBIRlUjf09Yafc7dXnOcDuTfDlX2HDHJj6FvQ5ofn3UoX/nuA0KE+4y3sxrpgOc+6DKz9x2koaU7QTHhnkJO9jr2vWW1pSMKaOTWuXEfLJzbQPq6ZDRBjREaHU+xUeFgkT/gQpx9fsKqmoYnVmAT07tqNbXBQA5VXVvPjxV8Qv/Q8lGsmH7S/hxFGpnDAggeT4dnSNiyQyLLThgKrK4cfnna6KZfkwYgoMOR++fRy2f4+GhCOuSrZ3HsvbsVeSGepUGwiQ0CGS7nHRJHSIYHNuMUu27WVlRh7lVS4SOkRyxpBuuFRZsj2Pn3MKatqPw3BxYeh8fhf2IR2lEIAboh/imkun8q/Z61iZns+Xt40nMSYSvrgHvv833LgUOvVxblBd5ZQs1s+CC1+AYRftf54Pr4N1M+H29U6DamQMlVfNprLaRdgPTxEx9x7o2Af+byGERzXzU2wCl8tpVG6tVD1ryN+nIBNim1d1BJYUTIDKyCululrp2Sn6gL/KdxWVU1xeRe/O7eu9btZPWex593dcJF+y0DUYcKoLosJCiAwPJTIshBD3/RLLthJVXci0/v9hR9QA1mQWsCqzgEhXKbFSwsie8ZzcP46wJS9wVtknSEgooVRTTgTPVp7Fu1UnUu0eAtS5QwRdY6PoFhdJfHQE4k5DfYuWcnLWc3SszGZDzDF83uO3ZEcPoKLKRVZ+Kb12fc2o4u94r+p4FriG0D0uinYRToJxKeQUlFFS4YzzDA0RhvSIZXTvjpw8qCtj+nU+oNqosKyS/NLKmu2YyHBipRhZ+DRbi0K5dFUaWQVlqMJfJw3hF2NSnBMLsuDxYTDyUqfkAvDlg04XzjMfhqN/feA/8uavnGkZxt7ilHzOehSOuto5Vl3p9PQZcgEkj27SZ268w5KCaXMKyyoJDw0hKnz/X9f5JZV8tjqLbzfuZvHWPWTlOyM7EzpEkta7Ix2iwliybS9bdhUDMKhbDJNGJjH+iEQiw5wv5o+WZ/LM3LUsjb6e8IEns2fisyzetpcVO/LI2FtKZn4pWfllNb1eEnU3L7nuJopyrg55gJjEZK4L+4Rjst8itLq0JrZqQsjucyFJ59/vDH6aex+s/djj511LH54I+QU/yvCafWEhQve4KHrER5PcMZqRPTuSltKRrrEH/mWtqhSUVpFTWEZSfDTtI5vfuzy/tJK/frKG/NJKnr189IHtEB9d7ySB2sbdARPuPPhGrmoniRRkQGgE3LYe2nVqdlzGuywpmDajtKKaZ7/exLNfb8KlypAecYzu3ZHte0r4av1OKquVrrGRHJXSiaNSOhEWKizZupdF2/ZQUl7Nkb07Mq5rBV1L1vNsZj+W7Cg86D3+MmALV+34E1z2Hgw4tfGgdm10ulqGRkB1BZTscv7K7TsOVcgtKiO63wnE9Bxy4HUZSyD7p8bv36EbDDitdVdvgDPKdt0noO6eQR26Og3hDVV7zLnXKSUMPhcmv9ZiYZrGWVIwrZKq8s2GXewtqQCgoLSSZ7/eTEZeKWcN707Pju1YvHUPK9Pz6dg+nHOG92DSyCSGJsU23IjrqoYXToHMpdBlCDuPvYsFMrLmi6tT+wjGLrkZ2fEj3LoWQj38qzpzmVMd0m04nHofJFm1R6N2bXR660x90xmNa1oNT5OCL6e5MOYAuYXl3PH+Suau23nA/kHdYph+7bEc23f/oJzKahehIoTU16WyriUvOQnh6Gvh59l0mXEZk/qfAhe/4nTtLNkDb82GY37jeUIA6DEKfr+5adcEu4T+cGd66y8BmQbZf+2mRcxZk8Mf319JYXkVfz47lQlHOMP1Q0To2andQf3pw0M9/FIp2glz7nf63U/8J5z2ICx6Hj7/M7x9OVz6jjPLpKvS6d3TVJYQms4SQptm/8Ubr8kpKGPu2p0c3acj/bvEALBjTwkPfLqG2atzGNw9lremjGRg1xjvvennf3YGdZ31iFNdFBYBY66HqHj43//Bh79xBjt1HXrAYCJjTP0sKRiPqCp7iivIzCsjI6+UsspqRvSMJ6VzOyqqXUz7div//nJDTTfJ1O6xDE+O44NlGYSK8PvTj+CaE/ocut9+Q3auhZAwSBhw4P4t38DK6XDC7QcfG3UZlOx2ZqAEOO2BZjy1McHHkoJp0Kcrs3jrx+1k5pWSkVdKeT1z0yR0iCAiNITM/DJOTe3K9RP6s2z7Xv63PJPpi3Zw3sge3DFxcM2AryYr3gXTznB6v1z5iTNFAzjz1L/3K+iYAifcVv+1x9/kTCWw5GVnOmNjTKOs95Gp18vfbeHej9fQN6E9g3vEkuSelyYpPpoe8dGEhQrLtuexeOtecgrK+PWJfRk38MBpfSurXZ63DTTko+ud0kD7RHBVwa9mQ0R7ePE0Z477X82GxIGHvkd1lbUNmKBnvY+MRyqqXHz6UyZ7iysZ3bsjqT1ieearTTz6xc+cltqVJ6eOOmAwWW2DusUy9eheDd77sBPCtgXOwKnjb4aRlznjBl47DyJinBLELz9uPCGAJQRjmsD+bwlSxeVVvPXjdl74ZgvZBfvXf40IC6GiysUFRybxzwuHE3a4X+zNVV0Jn97qrHI17g9O6eDy9+Dlc5zpFy5716ZLMMYHLCkEoW825HL7uyvIKSjn2L6deOjCYRzRLYYl2/ayeOteesRHcc3Yvp6NEfCVhc/AzjUw5U0nIYAzeOzqz6GyFHoe5b/YjAlglhQC0IacQorKq0iKjyahQ2TNl3tZZTUPzVrHy99vpX+XDjx92ZGM7r1/bpqzh0dz9vDmz8LoNWUFMP9hGHA6DKqz8le3of6JyZggYUkhwMxdm8PVr+xviA8PFaLc3UArql2UV7m48rgU7pg4qMG2Ar9b+gqU5++fmdMY02IsKQSQ7btLuOXt5QzpEcutpw4kM7+MzLzSmpWlAE4e3IXja68s1dpUVTirTKWcAElH+jsaY4KOJYUAUVpRzW9eX4KI8Ozlo+nZqZ2/Q2qeVe9DYSac+6S/IzEmKFlSCACZeaX847N1rMsuYNovj2q7CUEVvn8SuqRC/1P8HY0xQcmSQhuVX1LJA5+u4ftNu8nIcxZ+ufmUAUwY1MXPkR2GjXOcHkfnPdu0ZQqNMV5jSaENqqhycd3rS1i8bQ+npXbjmhP6cFRKJ4Ymxfk7tKbZuxW+exKK3VNpZ62EmB4w9EK/hmVMMLOk0MaoKnd9+BMLNu/msckjOH9Usr9DgqZOlVKyx1msftELzkR3+xaFj+gAY292Zjo1xviFJYU25umvNvHeknRuOnlA60gIm+bBGxc58xI1hYTAqMth/J0Q2wrGRhhjAEsKbUJeSQUzf8rmf8sz+GHLHiaN7MEtpwxo/MKWsH6Ws47xib/3/BoJhcFnQ5fBvovLGNMsPk0KInIG8AQQCrygqg/VOd4LeAWId59zh6rO9GVMbYmq8sI3W/jn7HVUVit9E9tz26kDuXZc34bXK25pOxY600/YQDNjAoLPkoKIhAJPAacC6cAiEZmhqmtqnXY38I6qPiMiqcBMIMVXMbUlZZXV3PXBT3ywLIPTUrty08kDGNLjEIvX+0N5EWSvghNu9Xckxhgv8WVJ4Whgo6puBhCR6cAkoHZSUCDW/ToOyPRhPG3GnuIKrnp5ESt25HHLKQO58aT+/p2criEZS0Croeex/o7EGOMlvkwKScCOWtvpwDF1zrkX+FxEbgTaA/WOWBKRa4FrAXr1anj+/kDx3/mbWJWRz7OXj+aMod38HU7DdvwACCQ3um6HMaaN8NNk+TWmAi+rajJwJvCaiBwUk6o+p6ppqpqWmJh40E0CiculzFieyfiBia07IQBsX+g0FkfH+zsSY4yX+DIpZAA9a20nu/fVdjXwDoCqLgCigFY8W5vv/bBlD1n5ZUwaleTvUA7N5YL0RdCzbuHPGNOW+TIpLAIGiEgfEYkApgAz6pyzHTgZQEQG4ySFXB/G1Op9tCyD9hGhnDq4q79DObTctVBeAL2sPcGYQOKzpKCqVcANwGxgLU4vo9Uicr+InOs+7Tbg1yKyAngLuFK1qcNjA0dZZTUzV2Vx+tBuREe00rUO9tm+0Pnd82j/xmGM8SqfjlNwjzmYWWffPbVerwGO92UMbclX63dSWFbFeSNbedUROI3M7btAxz7+jsQY40X+bmg2tXy4LIPEmEiO69fZf0EU73YmqmvMjh+g1zE2m6kxAcaSQiuRX1LJvHW5nDO8B2GhfvpYCrLg+fHw5CiYcaOzXZ/CHCdxWCOzMQHH5j5qBTLySnl49noqql2c769eR6V74fULnJLCqMth+Vuw8l1IPRfCog48tzDb+W2D1owJOJYU/CinoIx/fLaOGcudgdxXHpfC0KTYRq7ygYoSeHMK7N4Il74D/SbA2Fth3oOw+ev6r0lKg+4jWjZOY4zPWVLwo3v+t4qv1udyxZgUrj6hD0nx0S0fRPpimPVHZ8qKi192EgI4axxc+ELLx2OM8StLCn6ys7CMuWt38quxfbjrzBaaQnrPZsh3jx90VcKSl2HN/5xeRBe+AEPOa5k4jDGtliUFP/lgaQZVLuWStJ6Nn3y48tNh3t9g+Zs4cxC6hbd3FrkZcwNEdvB9HMaYVs+Sgh+oKu8s2sFRKR3p38WHX8alefDtY/DDs6AuGHM9DDwdcHcj7TIY2gf1rCLGmDosKfjBoq172byrmP+b0N83b1BV7qx/PP9fTmIYfglM+BN07O2b9zPGBAxLCn4wfdF2YiLDOHOYl2ZBLcyGt6ZCgXs5isoSZ16ivhPg1Pusl5AxxmOWFFpYfmklM3/K4oIjk2kX4aV//tl/gpxVMGIKICAhMPgc6H+yd+5vjAkalhRawFPzNvLxikx6xEdTWe2irNLFlKO81MC8aR6seg/G3QET7vTOPY0xQcuSgo+5XMq0b7cQFR5Kdn4ZmfmlHNevM8OS4g7/5lXlMPN2Z1K6sbcc/v2MMUHPkoKP/ZSRz+7iCh6fPJLzvD2FxXdPOqOQL3sfwqMaP98YYxphE+L52FfrcxGBEwd6eRnRPVvgm4chdRIMqHdpa2OMaTJLCj721c87GZ4cT6f2Ed67qSrM+gOEhMHpf/fefY0xQc+Sgg/tLa5g+Y48xnu7lLD2Y9jwOUy4C+LawII8xpg2w5KCD83fkIsqTBjUxXs3LS+Cz+6ArkPh6N94777GGIM1NPvU1+tz6dQ+guHe6GlUc9OHoCADLnoJQu3jM8Z4l5UUfMTlUr7+OZcTByQQEuKlJStzf4YFT8ORVzhLYRpjjJdZUvCRVZlOV9TxR3ix6ui7xyEsEk7+i/fuaYwxtVhS8BGvd0UtyISV78CoX9jMpsYYn7Gk4CNfrvNyV9QfngWthjH/5537GWNMPSwp+EBOQRnLd+Rx6mAvVR2VFcDilyD1POiY4p17GmNMPSwp+MDnq7MBOH2Il6bGXvKyMxX28Td5537GGNMASwo+MHt1Dn0T23tnVbWqClj4DPQ5EXqMOvz7GWPMIVhS8LL8kkoWbt7N6UO6IXKYXVF3LIJXz4XCTDj+d94J0BhjDsFGP3nZ3HU5VLn08KqOinLh01th7Qxo3wXOeQL62YI5xhjf82lSEJEzgCeAUOAFVX2ozvHHgAnuzXZAF1WN92VMvvbZqmy6xUYd3ijmT26GDV/A+DthzA0Q6YVqKGOM8YDPkoKIhAJPAacC6cAiEZmhqmv2naOqt9Q6/0agTVeal1ZUM39DLpPTejZ/FPP6z2DdJ3DKvbZwjjGmxfmyTeFoYKOqblbVCmA6MOkQ508F3vJhPD739c+5lFW6ml91VFECs34PiYPg2Ou9G5wxxnjAl0khCdhRazvdve8gItIb6AN82cDxa0VksYgszs3N9Xqg3jJ7dTbx7cI5uk+n5t3gm4chbzuc9QiEeXH9BWOM8VBr6X00BXhPVavrO6iqz6lqmqqmJSZ6eW0CL6msdjF3bQ4nD+pKWGgz/llzf3aW1xwxFVLGej9AY4zxgC+TQgbQs9Z2sntffabQxquOFm7eTUFZFWcMbWbV0Q/PQmg4nPpX7wZmjDFN4MuksAgYICJ9RCQC54t/Rt2TRGQQ0BFY4MNYfG726mzaRYRywoBmTFanCutnQf+ToUPrLAkZY4JDo0lBRG4UkY5NvbGqVgE3ALOBtcA7qrpaRO4XkXNrnToFmK6q2tT3aC1cLuXz1TmMG5hIVHho02+QtdwZoHbEmd4PzhhjmsCTLqldcbqTLgWmAbM9/QJX1ZnAzDr77qmzfa9nobZey3bksbOwvPm9jtbPAgmBAad7NzBjjGmiRksKqno3MAB4EbgS2CAifxORfj6Orc34fHU2YSHS/LWY18+EnsdC+87eDcwYY5rIozYFd8kg2/1ThdMG8J6I/NOHsbUJqsrs1dmM6deZuOjwpt8gbztk/wRHTPR+cMYY00SetCn8TkSWAP8EvgOGqepIFQnGAAAXt0lEQVRvgdHAhT6Or9X7OaeIrbtLDqPq6DPnt7UnGGNaAU/aFDoBF6jqtto7VdUlImf7Jqy2Y/bqbETgtNSuzbvB+pmQMBAS+ns3MGOMaQZPqo9mAXv2bYhIrIgcA6Cqa30VWFsxe3U2R/bqSJfYqKZfXJYPW7+1qiNjTKvhSVJ4BiiqtV3k3hf0ql3KuuxCjmnutBYb54Cr0qqOjDGthidJQWp3QVVVF7YOAwC5heVUu5Qe8dFNv7ggC+bcC7FJkHyU12Mzxpjm8CQpbBaRm0Qk3P3zO2CzrwNrC7LySwHoEd/EqqPSvfD6BVCyBya/DiHNGPBmjDE+4ElSuA44DmfeonTgGOBaXwbVVmTllwHQLbYJJYWKEnhzMuzeCFPegKQjfRSdMcY0XaPVQKq6E2cqClPHvqTQpJLC53fDjh/h4peh73hfhGWMMc3WaFIQkSjgamAIUPPtp6q/8mFcbUJWXilR4SGeD1qrKIGVb8PIS2HIeb4NzhhjmsGT6qPXgG7A6cDXOFNgF/oyqLYiq6CMHnHRiHi49Oa6T6GiyFkzwRhjWiFPkkJ/Vf0zUKyqrwBn4bQrBL2svFK6xTWh6mjldIjrCb2P911QxhhzGDxJCpXu33kiMhSIA5o581tgyc4vo3uch43Mhdmw6UsYPhlCWsuCd8YYcyBPxhs8515P4W6cRXI6AH/2aVRtQLVLySksp7unJYWf3gV1wQhrszfGtF6HTAoiEgIUqOpeYD7Qt0WiagP2DVzr7mnPoxXTISkNEgb4NjBjjDkMh6zHcI9e/kMLxdKmZLoHrnlUUsj+CXJWWSnBGNPqeVK5PUdEbheRniLSad+PzyNr5bLdYxQ8alNY/haEhMPQoJ9p3BjTynnSpjDZ/fv6WvuUIK9KyszzsKRQXgjLXodBZ0G7oM+lxphWzpMRzX1aIpC2Jju/zLOBa0tfhfJ8OO6mlgnMGGMOgycjmq+ob7+qvur9cNqOrHwPBq5VV8KCp6H3WEge3XLBGWNMM3lSfVR7Xuco4GRgKRDkScGDgWurPoCCdDj70ZYJyhhjDpMn1Uc31t4WkXhgus8iaiOy8ss4rl9CwyeowvdPQuJg6H9qywVmjDGHoTlDa4uBoG5nqKp2sbOxgWub5jrdUI+70UYwG2PaDE/aFD7G6W0EThJJBd7xZVCtXW6RBwPXFjwFMd1h2MUtF5gxxhwmT9oUHq71ugrYpqrpPoqnTciqGaPQQFLIT4dN82DcHyEsogUjM8aYw+NJUtgOZKlqGYCIRItIiqpu9WlkrVijA9dWvgOojWA2xrQ5nlR2vwu4am1Xu/cFrUMOXFN15jnqNQY6BXXTizGmDfIkKYSpasW+Dfdrj+pEROQMEVkvIhtF5I4GzrlERNaIyGoRedOzsP0rO7+M6PDQ+geuZS6DXeutlGCMaZM8qT7KFZFzVXUGgIhMAnY1dpGIhAJPAacC6cAiEZmhqmtqnTMAuBM4XlX3ikibWKchK7+M7nFR9Q9cWzEdQiMh1ZbbNMa0PZ4kheuAN0TkP+7tdKDeUc51HA1sVNXNACIyHZgErKl1zq+Bp9xTc6OqOz0N3J+y8kvr73lUVQGr3oNBZ0J0fMsHZowxh8mTwWubgGNFpIN7u8jDeycBO2ptp3PwMp4DAUTkOyAUuFdVP6t7IxG5FrgWoFevXh6+ve80OHBt4xwo2W1rMBtj2qxG2xRE5G8iEq+qRapaJCIdReQBL71/GDAAGA9MBZ53j5g+gKo+p6ppqpqWmJjopbdunkMOXFvxFrRPhH4ntXxgxhjjBZ40NE9U1bx9G+6qnjM9uC4D6FlrO9m9r7Z0YIaqVqrqFuBnnCTRau3YW0q1S+ndud2BB1Rhy9dwxEQIbWTmVGOMaaU8SQqhIhK5b0NEooHIQ5y/zyJggIj0EZEIYArOGs+1fYRTSkBEEnCqkzZ7cG+/2bLLqT3rm9jhwANFOVCWD12H+iEqY4zxDk8amt8A5orIS4AAVwKvNHaRqlaJyA3AbJz2gmmqulpE7gcWu3szzQZOE5E1OOMffq+qu5v3KC1jc24xAH0T2h94YOda53fioBaOyBhjvMeThuZ/iMgK4BScOZBmA709ubmqzgRm1tl3T63XCtzq/mkTNu8qpmO7cDq2rzNUI3ed87vL4JYPyhhjvMTT6TtzcBLCxcBJwFqfRdTKbc4tok/dUgI4JYXojk5DszHGtFENlhREZCBOj6CpOIPV3gZEVSe0UGyt0pZdxZwwoJ4v/tz1ztoJh1qJzRhjWrlDlRTW4ZQKzlbVsar6b5x6/6BVVF5FTkH5wSUFVchdC12sPcEY07YdKilcAGQB80TkeRE5GaehOWht3eU0MvdLrJMUCrOdnkeJ1p5gjGnbGkwKqvqRqk4BBgHzgJuBLiLyjIic1lIBtiabcp3uqH0S6nRH3dfInHhEC0dkjDHe1WhDs6oWq+qbqnoOzgC0ZcAffR5ZK7RlVzEiHDxwzXoeGWMCRJMWD1bVve4pJ072VUCt2ebcYpLio4kKDz3wwM61EN3Jeh4ZY9o8W1G+CbbsKj54JDM4JYXEQdbzyBjT5llS8JCqOkmh3p5H66znkTEmIFhS8FBuYTlF5VUHd0e1nkfGmABiScFDm93dUfvW7Y6au2/OI+t5ZIxp+ywpeGjfRHgHlRRy1zu/reeRMSYAWFLw0JZdRUSGhdAjLvrAA9bzyBgTQCwpeGhzbjF9EtoTElKnh1HuOqeUYD2PjDEBwJKCh5zuqA30PLL2BGNMgLCk4IHKahfb95RYzyNjTMCzpOCB7zbuosqlDEuKP/DAvp5HNkbBGBMgLCl44O1FO+jUPoKTBnU58MDOfRPhWVIwxgQGSwqN2FVUzpy1OVx4ZBIRYXX+uXLXWc8jY0xAsaTQiA+XZlBZrUw+qufBB63nkTEmwFhSOARVZfqi7Yzu3ZH+XWLqHnSqj6zqyBgTQCwpHMKSbXvZlFvM5LR6SgmFWVCeb0nBGBNQLCkcwtuLdtA+IpSzhnc/+GDNwjqWFIwxgSPM3wG0NqUV1SzfkcfirXv4ZGUW543qQfvIev6Zanoe2RgFY0zgCLqkUFZZzb9mr+fVBVuprNZDnpvaPZZrT+xX/8HcfXMeJXg/SGOM8ZOgSgprMgu4+e1l/JxTxAWjkkju1O6gc8JChKFJsRzZqyPx7SIavlnueut5ZIwJOEGTFN5ZtIO7P1pFXLtwXr7qKMYf0aXxixqyr+fRsIu8F6AxxrQCPm1oFpEzRGS9iGwUkTvqOX6liOSKyHL3zzW+iqVflw6cktqF2TefeHgJAaznkTEmYPmspCAiocBTwKlAOrBIRGao6po6p76tqjf4Ko59RvfuyOjeo71zM+t5ZIwJUL4sKRwNbFTVzapaAUwHJvnw/VqO9TwyxgQoXyaFJGBHre109766LhSRlSLynojUM0oMRORaEVksIotzc3N9EWvT5K6Fdp2hg815ZIwJLP4evPYxkKKqw4EvgFfqO0lVn1PVNFVNS0xsBV/ENr2FMSZA+TIpZAC1//JPdu+roaq7VbXcvfkC4KVKfx9SdbqjWlIwxgQgXyaFRcAAEekjIhHAFGBG7RNEpPb8EecCa30Yj3eU7nV6HnXq6+9IjDHG63zW+0hVq0TkBmA2EApMU9XVInI/sFhVZwA3ici5QBWwB7jSV/F4TUGm8zuuvuYRY4xp23w6eE1VZwIz6+y7p9brO4E7fRmD1xVmOb9jevg3DmOM8QF/NzS3PQXuZpFYSwrGmMBjSaGpCjIBgZhu/o7EGGO8zpJCUxVkQocuEBru70iMMcbrLCk0VUGmVR0ZYwKWJYWmKsiEWOt5ZIwJTJYUmqowE2LqWZ7TGGMCgCWFpqgohrJ8qz4yxgQsSwpNUeAeo2DVR8aYAGVJoSlqxihY9ZExJjBZUmiKfVNcWEnBGBOgLCk0RaE7KVhDszEmQFlSaIqCTIiKh4h2/o7EGGN8wpJCU9gYBWNMgLOk0BQ2mtkYE+AsKTRFQab1PDLGBDRLCp6qqoDinVZ9ZIwJaD5dZCegFGU7v63nkTFtRmVlJenp6ZSVlfk7lBYTFRVFcnIy4eHNm8nZkoKnbIyCMW1Oeno6MTExpKSkICL+DsfnVJXdu3eTnp5Onz59mnUPqz7ylK24ZkybU1ZWRufOnYMiIQCICJ07dz6skpElBU/VzHtk1UfGtCXBkhD2OdzntaTgqYJMCG/nDF4zxpgAZUnBUwUZTtVRkP3VYYxpvt27dzNy5EhGjhxJt27dSEpKqtmuqKjw6B5XXXUV69ev93Gk+1lDs6cKs6znkTGmSTp37szy5csBuPfee+nQoQO33377AeeoKqpKSEj9f6O/9NJLPo+zNksKnirIhN7H+zsKY0wz3ffxatZkFnj1nqk9YvnLOUOafN3GjRs599xzGTVqFMuWLeOLL77gvvvuY+nSpZSWljJ58mTuueceAMaOHct//vMfhg4dSkJCAtdddx2zZs2iXbt2/O9//6NLly5efSarPvKEy+WUFKznkTHGS9atW8ctt9zCmjVrSEpK4qGHHmLx4sWsWLGCL774gjVr1hx0TX5+PuPGjWPFihWMGTOGadOmeT0uKyl4ojATXFUQZ2MUjGmrmvMXvS/169ePtLS0mu233nqLF198kaqqKjIzM1mzZg2pqakHXBMdHc3EiRMBGD16NN98843X47Kk4ImsFc7vbsP9G4cxJmC0b9++5vWGDRt44okn+PHHH4mPj+fyyy+vd6xBREREzevQ0FCqqqq8HpdVH3kicxlIKHQd6u9IjDEBqKCggJiYGGJjY8nKymL27Nl+i8WnSUFEzhCR9SKyUUTuOMR5F4qIikhaQ+f4VcZS6JJqi+sYY3ziyCOPJDU1lUGDBnHFFVdw/PH+69QiquqbG4uEAj8DpwLpwCJgqqquqXNeDPApEAHcoKqLD3XftLQ0Xbz4kKd4lyr8sy8MOgsm/afl3tcYc9jWrl3L4MGD/R1Gi6vvuUVkiao2+oe3L0sKRwMbVXWzqlYA04FJ9Zz3V+AfQOucxjBvO5TugR6j/B2JMcb4nC+TQhKwo9Z2untfDRE5Euipqp8e6kYicq2ILBaRxbm5ud6P9FAylzm/LSkYY4KA3xqaRSQEeBS4rbFzVfU5VU1T1bTExETfB1db5jIIjYCuras7mzHG+IIvk0IG0LPWdrJ73z4xwFDgKxHZChwLzGh1jc2Zy5yEEBbp70iMMcbnfJkUFgEDRKSPiEQAU4AZ+w6qar6qJqhqiqqmAAuBcxtraG5RLhdkLreqI2NM0PBZUlDVKuAGYDawFnhHVVeLyP0icq6v3ter9m6B8nxLCsaYoOHTNgVVnamqA1W1n6o+6N53j6rOqOfc8a2qlADWyGyMOSwTJkw4aCDa448/zm9/+9sGr+nQoYOvwzokG9F8KJnLICwKEoOvn7Mx5vBNnTqV6dOnH7Bv+vTpTJ061U8RNc7mPjqUzGXOfEeh9s9kTJs36w7I/sm79+w2DCY+1ODhiy66iLvvvpuKigoiIiLYunUrmZmZjBo1ipNPPpm9e/dSWVnJAw88wKRJ9Q3janlWUmiIq9qZCM+qjowxzdSpUyeOPvpoZs2aBTilhEsuuYTo6Gg+/PBDli5dyrx587jtttvw1ewSTWV/Ajdk+RtQUQTJrauHrDGmmQ7xF70v7atCmjRpEtOnT+fFF19EVbnrrruYP38+ISEhZGRkkJOTQ7du3fwSY21WUqjPupnw8c3QdzyknufvaIwxbdikSZOYO3cuS5cupaSkhNGjR/PGG2+Qm5vLkiVLWL58OV27dq13qmx/sKRQ19bv4L2roPsImPwGhEU0fo0xxjSgQ4cOTJgwgV/96lc1Dcz5+fl06dKF8PBw5s2bx7Zt2/wc5X7BU3209DVY4MEsp3nbIb4XXPYeRPq3a5gxJjBMnTqV888/v6Yn0mWXXcY555zDsGHDSEtLY9CgQX6OcL/gSQrtOkHiEY2fl5QGE+6E9p19H5MxJiicd955BzQkJyQksGDBgnrPLSoqaqmw6hU8SWHQWc6PMcaYBlmbgjHGmBqWFIwxAa219P9vKYf7vJYUjDEBKyoqit27dwdNYlBVdu/eTVRUVLPvETxtCsaYoJOcnEx6ejotvmKjH0VFRZGcnNzs6y0pGGMCVnh4OH369PF3GG2KVR8ZY4ypYUnBGGNMDUsKxhhjakhba5UXkVyguROFJAC7vBhOWxGMzx2MzwzB+dzB+MzQ9OfuraqJjZ3U5pLC4RCRxaoadHNhB+NzB+MzQ3A+dzA+M/juua36yBhjTA1LCsYYY2oEW1J4zt8B+EkwPncwPjME53MH4zODj547qNoUjDHGHFqwlRSMMcYcgiUFY4wxNYImKYjIGSKyXkQ2isgd/o7HF0Skp4jME5E1IrJaRH7n3t9JRL4QkQ3u3x39Hau3iUioiCwTkU/c231E5Af35/22iATcYtsiEi8i74nIOhFZKyJjguSzvsX93/cqEXlLRKIC7fMWkWkislNEVtXaV+9nK44n3c++UkSOPJz3DoqkICKhwFPARCAVmCoiqf6NyieqgNtUNRU4Frje/Zx3AHNVdQAw170daH4HrK21/Q/gMVXtD+wFrvZLVL71BPCZqg4CRuA8f0B/1iKSBNwEpKnqUCAUmELgfd4vA2fU2dfQZzsRGOD+uRZ45nDeOCiSAnA0sFFVN6tqBTAdmOTnmLxOVbNUdan7dSHOl0QSzrO+4j7tFeA8/0ToGyKSDJwFvODeFuAk4D33KYH4zHHAicCLAKpaoap5BPhn7RYGRItIGNAOyCLAPm9VnQ/sqbO7oc92EvCqOhYC8SLSvbnvHSxJIQnYUWs73b0vYIlICjAK+AHoqqpZ7kPZQFc/heUrjwN/AFzu7c5AnqpWubcD8fPuA+QCL7mrzV4QkfYE+GetqhnAw8B2nGSQDywh8D9vaPiz9er3W7AkhaAiIh2A94GbVbWg9jF1+iAHTD9kETkb2KmqS/wdSwsLA44EnlHVUUAxdaqKAu2zBnDXo0/CSYo9gPYcXM0S8Hz52QZLUsgAetbaTnbvCzgiEo6TEN5Q1Q/cu3P2FSfdv3f6Kz4fOB44V0S24lQLnoRT1x7vrl6AwPy804F0Vf3Bvf0eTpII5M8a4BRgi6rmqmol8AHOfwOB/nlDw5+tV7/fgiUpLAIGuHsoROA0TM3wc0xe565LfxFYq6qP1jo0A/il+/Uvgf+1dGy+oqp3qmqyqqbgfK5fquplwDzgIvdpAfXMAKqaDewQkSPcu04G1hDAn7XbduBYEWnn/u9933MH9Oft1tBnOwO4wt0L6Vggv1Y1U5MFzYhmETkTp+45FJimqg/6OSSvE5GxwDfAT+yvX78Lp13hHaAXzrTjl6hq3UasNk9ExgO3q+rZItIXp+TQCVgGXK6q5f6Mz9tEZCRO43oEsBm4CucPvYD+rEXkPmAyTm+7ZcA1OHXoAfN5i8hbwHic6bFzgL8AH1HPZ+tOjv/BqUYrAa5S1cXNfu9gSQrGGGMaFyzVR8YYYzxgScEYY0wNSwrGGGNqWFIwxhhTw5KCMcaYGpYUjKlDRKpFZHmtH69NKiciKbVnvjSmtQlr/BRjgk6pqo70dxDG+IOVFIzxkIhsFZF/ishPIvKjiPR3708RkS/dc9nPFZFe7v1dReRDEVnh/jnOfatQEXnevSbA5yIS7beHMqYOSwrGHCy6TvXR5FrH8lV1GM4I0sfd+/4NvKKqw4E3gCfd+58EvlbVETjzEq127x8APKWqQ4A84EIfP48xHrMRzcbUISJFqtqhnv1bgZNUdbN74sFsVe0sIruA7qpa6d6fpaoJIpILJNeebsE9pfkX7oVSEJE/AuGq+oDvn8yYxllJwZim0QZeN0XtOXmqsbY904pYUjCmaSbX+r3A/fp7nBlaAS7DmZQQnCUTfws1a0jHtVSQxjSX/YVizMGiRWR5re3PVHVft9SOIrIS56/9qe59N+KsgPZ7nNXQrnLv/x3wnIhcjVMi+C3OamHGtFrWpmCMh9xtCmmqusvfsRjjK1Z9ZIwxpoaVFIwxxtSwkoIxxpgalhSMMcbUsKRgjDGmhiUFY4wxNSwpGGOMqfH/Qs+EhiUJlkUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['acc'])\n",
    "plt.plot(hist.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Regularization to our Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train a model which will overfit, which we call Model 2. This might take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1022 samples, validate on 219 samples\n",
      "Epoch 1/100\n",
      "1022/1022 [==============================] - 11s 11ms/step - loss: 0.4571 - acc: 0.7906 - val_loss: 0.3741 - val_acc: 0.8356\n",
      "Epoch 2/100\n",
      "1022/1022 [==============================] - 10s 10ms/step - loss: 0.3287 - acc: 0.8532 - val_loss: 0.2879 - val_acc: 0.9087\n",
      "Epoch 3/100\n",
      "1022/1022 [==============================] - 10s 10ms/step - loss: 0.3644 - acc: 0.8366 - val_loss: 0.3065 - val_acc: 0.8950\n",
      "Epoch 4/100\n",
      "1022/1022 [==============================] - 10s 10ms/step - loss: 0.2868 - acc: 0.8738 - val_loss: 0.3453 - val_acc: 0.8676\n",
      "Epoch 5/100\n",
      "1022/1022 [==============================] - 10s 10ms/step - loss: 0.2980 - acc: 0.8718 - val_loss: 0.3622 - val_acc: 0.8676\n",
      "Epoch 6/100\n",
      "1022/1022 [==============================] - 10s 10ms/step - loss: 0.2578 - acc: 0.8933 - val_loss: 0.4521 - val_acc: 0.7991\n",
      "Epoch 7/100\n",
      "1022/1022 [==============================] - 10s 10ms/step - loss: 0.2821 - acc: 0.8748 - val_loss: 0.3102 - val_acc: 0.8904\n",
      "Epoch 8/100\n",
      "1022/1022 [==============================] - 10s 10ms/step - loss: 0.2417 - acc: 0.8943 - val_loss: 0.3140 - val_acc: 0.8813\n",
      "Epoch 9/100\n",
      "1022/1022 [==============================] - 10s 10ms/step - loss: 0.2420 - acc: 0.8933 - val_loss: 0.3701 - val_acc: 0.8676\n",
      "Epoch 10/100\n",
      "1022/1022 [==============================] - 10s 10ms/step - loss: 0.2440 - acc: 0.8953 - val_loss: 0.3055 - val_acc: 0.9224\n",
      "Epoch 11/100\n",
      "1022/1022 [==============================] - 10s 10ms/step - loss: 0.2314 - acc: 0.8982 - val_loss: 0.2588 - val_acc: 0.9269\n",
      "Epoch 12/100\n",
      "1022/1022 [==============================] - 10s 10ms/step - loss: 0.2208 - acc: 0.8982 - val_loss: 0.3215 - val_acc: 0.8813\n",
      "Epoch 13/100\n",
      "1022/1022 [==============================] - 10s 10ms/step - loss: 0.2470 - acc: 0.8973 - val_loss: 0.3177 - val_acc: 0.8950\n",
      "Epoch 14/100\n",
      "1022/1022 [==============================] - 11s 10ms/step - loss: 0.2246 - acc: 0.8982 - val_loss: 0.3160 - val_acc: 0.8858\n",
      "Epoch 15/100\n",
      "1022/1022 [==============================] - 11s 11ms/step - loss: 0.2238 - acc: 0.9002 - val_loss: 0.3013 - val_acc: 0.8995\n",
      "Epoch 16/100\n",
      "1022/1022 [==============================] - 13s 13ms/step - loss: 0.2125 - acc: 0.9041 - val_loss: 0.2975 - val_acc: 0.9041\n",
      "Epoch 17/100\n",
      "1022/1022 [==============================] - 14s 14ms/step - loss: 0.2083 - acc: 0.9041 - val_loss: 0.2720 - val_acc: 0.9132\n",
      "Epoch 18/100\n",
      "1022/1022 [==============================] - 14s 14ms/step - loss: 0.2060 - acc: 0.9070 - val_loss: 0.3559 - val_acc: 0.8721\n",
      "Epoch 19/100\n",
      "1022/1022 [==============================] - 11s 11ms/step - loss: 0.2163 - acc: 0.9041 - val_loss: 0.2948 - val_acc: 0.8950\n",
      "Epoch 20/100\n",
      "1022/1022 [==============================] - 11s 11ms/step - loss: 0.2057 - acc: 0.9080 - val_loss: 0.3008 - val_acc: 0.9406\n",
      "Epoch 21/100\n",
      "1022/1022 [==============================] - 11s 11ms/step - loss: 0.2385 - acc: 0.9031 - val_loss: 0.3601 - val_acc: 0.8539\n",
      "Epoch 22/100\n",
      "1022/1022 [==============================] - 12s 12ms/step - loss: 0.2168 - acc: 0.9090 - val_loss: 0.2771 - val_acc: 0.9087\n",
      "Epoch 23/100\n",
      "1022/1022 [==============================] - 14s 13ms/step - loss: 0.1993 - acc: 0.9119 - val_loss: 0.3582 - val_acc: 0.8539\n",
      "Epoch 24/100\n",
      "1022/1022 [==============================] - 12s 12ms/step - loss: 0.2164 - acc: 0.9012 - val_loss: 0.3283 - val_acc: 0.8813\n",
      "Epoch 25/100\n",
      "1022/1022 [==============================] - 13s 12ms/step - loss: 0.1956 - acc: 0.9129 - val_loss: 0.2904 - val_acc: 0.9269\n",
      "Epoch 26/100\n",
      "1022/1022 [==============================] - 11s 11ms/step - loss: 0.1951 - acc: 0.9080 - val_loss: 0.2916 - val_acc: 0.9361\n",
      "Epoch 27/100\n",
      "1022/1022 [==============================] - 14s 14ms/step - loss: 0.2129 - acc: 0.9090 - val_loss: 0.2823 - val_acc: 0.9087\n",
      "Epoch 28/100\n",
      "1022/1022 [==============================] - 13s 13ms/step - loss: 0.1903 - acc: 0.9159 - val_loss: 0.3071 - val_acc: 0.9087\n",
      "Epoch 29/100\n",
      "1022/1022 [==============================] - 13s 12ms/step - loss: 0.1904 - acc: 0.9178 - val_loss: 0.3173 - val_acc: 0.9269\n",
      "Epoch 30/100\n",
      "1022/1022 [==============================] - 11s 11ms/step - loss: 0.2094 - acc: 0.9070 - val_loss: 0.3538 - val_acc: 0.9498\n",
      "Epoch 31/100\n",
      "1022/1022 [==============================] - 11s 11ms/step - loss: 0.1928 - acc: 0.9100 - val_loss: 0.3699 - val_acc: 0.9269\n",
      "Epoch 32/100\n",
      "1022/1022 [==============================] - 11s 11ms/step - loss: 0.1884 - acc: 0.9217 - val_loss: 0.3396 - val_acc: 0.9178\n",
      "Epoch 33/100\n",
      "1022/1022 [==============================] - 11s 11ms/step - loss: 0.1872 - acc: 0.9168 - val_loss: 0.3957 - val_acc: 0.9087\n",
      "Epoch 34/100\n",
      "1022/1022 [==============================] - 11s 11ms/step - loss: 0.2081 - acc: 0.9168 - val_loss: 0.2865 - val_acc: 0.9269\n",
      "Epoch 35/100\n",
      "1022/1022 [==============================] - 11s 11ms/step - loss: 0.1841 - acc: 0.9178 - val_loss: 0.3380 - val_acc: 0.8995\n",
      "Epoch 36/100\n",
      "1022/1022 [==============================] - 11s 11ms/step - loss: 0.1805 - acc: 0.9237 - val_loss: 0.3675 - val_acc: 0.8721\n",
      "Epoch 37/100\n",
      "1022/1022 [==============================] - 11s 11ms/step - loss: 0.1889 - acc: 0.9237 - val_loss: 0.4068 - val_acc: 0.8721\n",
      "Epoch 38/100\n",
      "1022/1022 [==============================] - 11s 11ms/step - loss: 0.2017 - acc: 0.9080 - val_loss: 0.2972 - val_acc: 0.9224\n",
      "Epoch 39/100\n",
      "1022/1022 [==============================] - 11s 11ms/step - loss: 0.1778 - acc: 0.9207 - val_loss: 0.3345 - val_acc: 0.9041\n",
      "Epoch 40/100\n",
      "1022/1022 [==============================] - 11s 11ms/step - loss: 0.1863 - acc: 0.9227 - val_loss: 0.3370 - val_acc: 0.9361\n",
      "Epoch 41/100\n",
      "1022/1022 [==============================] - 11s 11ms/step - loss: 0.1737 - acc: 0.9198 - val_loss: 0.3667 - val_acc: 0.8950\n",
      "Epoch 42/100\n",
      "1022/1022 [==============================] - 11s 11ms/step - loss: 0.1741 - acc: 0.9227 - val_loss: 0.3472 - val_acc: 0.8858\n",
      "Epoch 43/100\n",
      "1022/1022 [==============================] - 11s 11ms/step - loss: 0.1854 - acc: 0.9149 - val_loss: 0.3035 - val_acc: 0.8995\n",
      "Epoch 44/100\n",
      "1022/1022 [==============================] - 13s 13ms/step - loss: 0.1748 - acc: 0.9198 - val_loss: 0.4062 - val_acc: 0.9269\n",
      "Epoch 45/100\n",
      "1022/1022 [==============================] - 13s 13ms/step - loss: 0.1708 - acc: 0.9276 - val_loss: 0.3807 - val_acc: 0.8721\n",
      "Epoch 46/100\n",
      "1022/1022 [==============================] - 11s 11ms/step - loss: 0.1780 - acc: 0.9217 - val_loss: 0.3450 - val_acc: 0.8995\n",
      "Epoch 47/100\n",
      "1022/1022 [==============================] - 11s 11ms/step - loss: 0.1662 - acc: 0.9247 - val_loss: 0.4028 - val_acc: 0.8995\n",
      "Epoch 48/100\n",
      "1022/1022 [==============================] - 11s 11ms/step - loss: 0.1776 - acc: 0.9207 - val_loss: 0.4083 - val_acc: 0.8630\n",
      "Epoch 49/100\n",
      "1022/1022 [==============================] - 12s 11ms/step - loss: 0.2011 - acc: 0.8973 - val_loss: 0.3421 - val_acc: 0.8904\n",
      "Epoch 50/100\n",
      "1022/1022 [==============================] - 13s 13ms/step - loss: 0.1725 - acc: 0.9168 - val_loss: 0.4079 - val_acc: 0.8767\n",
      "Epoch 51/100\n",
      "1022/1022 [==============================] - 13s 13ms/step - loss: 0.1803 - acc: 0.9207 - val_loss: 0.4080 - val_acc: 0.9224\n",
      "Epoch 52/100\n",
      "1022/1022 [==============================] - 12s 11ms/step - loss: 0.1850 - acc: 0.9217 - val_loss: 0.3654 - val_acc: 0.9041\n",
      "Epoch 53/100\n",
      "1022/1022 [==============================] - 13s 13ms/step - loss: 0.1652 - acc: 0.9295 - val_loss: 0.3448 - val_acc: 0.9269\n",
      "Epoch 54/100\n",
      "1022/1022 [==============================] - 11s 11ms/step - loss: 0.1587 - acc: 0.9315 - val_loss: 0.3473 - val_acc: 0.9269\n",
      "Epoch 55/100\n",
      "1022/1022 [==============================] - 11s 11ms/step - loss: 0.1723 - acc: 0.9276 - val_loss: 0.3903 - val_acc: 0.8813\n",
      "Epoch 56/100\n",
      "1022/1022 [==============================] - 15s 15ms/step - loss: 0.1718 - acc: 0.9237 - val_loss: 0.4002 - val_acc: 0.9361\n",
      "Epoch 57/100\n",
      " 992/1022 [============================>.] - ETA: 0s - loss: 0.2163 - acc: 0.9093"
     ]
    }
   ],
   "source": [
    "model_2 = Sequential([\n",
    "    Dense(1000, activation='relu', input_shape=(10,)),\n",
    "    Dense(1000, activation='relu'),\n",
    "    Dense(1000, activation='relu'),\n",
    "    Dense(1000, activation='relu'),\n",
    "    Dense(1, activation='sigmoid'),\n",
    "])\n",
    "model_2.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "hist_2 = model_2.fit(X_train, Y_train,\n",
    "          batch_size=32, epochs=100,\n",
    "          validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same visualization to see what overfitting looks like in terms of the loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist_2.history['loss'])\n",
    "plt.plot(hist_2.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist_2.history['acc'])\n",
    "plt.plot(hist_2.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address the overfitting we see in Model 2, we'll incorporate L2 regularization and dropout in our third model here (Model 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = Sequential([\n",
    "    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01), input_shape=(10,)),\n",
    "    Dropout(0.3),\n",
    "    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.3),\n",
    "    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.3),\n",
    "    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "hist_3 = model_3.fit(X_train, Y_train,\n",
    "          batch_size=32, epochs=100,\n",
    "          validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now plot the loss and accuracy graphs for Model 3. You'll notice that the loss is a lot higher at the start, and that's because we've changed our loss function. To plot such that the window is zoomed in between 0 and 1.2 for the loss, we add an additional line of code (plt.ylim) when plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist_3.history['loss'])\n",
    "plt.plot(hist_3.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.ylim(top=1.2, bottom=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist_3.history['acc'])\n",
    "plt.plot(hist_3.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As compared to Model 2, you should see that there's less overfitting!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
